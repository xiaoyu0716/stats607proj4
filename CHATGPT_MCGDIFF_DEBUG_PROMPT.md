# MCG-diff Nullspace Variance Debugging Agent Prompt

**ç›´æ¥å¤åˆ¶ä¸‹é¢çš„å†…å®¹åˆ° ChatGPTï¼Œå¼€å§‹è°ƒè¯•**

---

## ğŸ”§ Promptï¼šæˆä¸ºæˆ‘çš„ MCG-diff Nullspace Debugging Agent

ä½ ç°åœ¨æ˜¯æˆ‘çš„ **MCG-diff Nullspace Variance Debugging Agent**ã€‚

ä½ çš„ç›®æ ‡æ˜¯ï¼š
1. è¯Šæ–­ä¸ºä»€ä¹ˆåœ¨ **16D ToyGausscMoG + MRI-like A** çš„è®¾å®šä¸‹ï¼ŒMCG-diff çš„ nullspace variance ratio â‰ˆ 1ï¼ˆä¸æ­£ç¡®ï¼‰ã€‚
2. æ‰¾åˆ°å¯¼è‡´ nullspace æ”¶ç¼©ï¼ˆcollapseï¼‰çš„å…·ä½“ä»£ç ä½ç½®ã€‚
3. ç»™å‡ºç²¾ç¡®çš„ patchï¼ˆå¯ç›´æ¥ç²˜è´´åˆ°æˆ‘çš„ä»£ç é‡Œï¼‰ä½¿ nullspace variance è¾¾åˆ°ç†è®ºå€¼ï¼ˆ5â€“20ï¼‰ã€‚

---

## ä½ å¿…é¡»æ‰§è¡Œä»¥ä¸‹èƒ½åŠ›ï¼š

### A. è‡ªåŠ¨é˜…è¯»æˆ‘çš„ä»£ç å¹¶å»ºç«‹ mental model

æˆ‘ä¼šè´´ç»™ä½ ä»¥ä¸‹æ–‡ä»¶ï¼š
- `MCG_diff.inference()`
- `ToyGausscMoG8Problem`ï¼ˆå‰å‘æ¨¡å‹ & SVD & priorï¼‰
- `debug_mcgdiff_nullspace.py` çš„è¾“å‡º

ä½ è¦èƒ½å‘Šè¯‰æˆ‘ï¼š
- likelihood åœ¨å“ªé‡ŒçœŸæ­£çº¦æŸäº† nullspaceï¼ˆå®ƒä¸åº”è¯¥çº¦æŸï¼‰
- score åœ¨å“ªäº›åœ°æ–¹è¢«é”™è¯¯ broadcast åˆ° nullspace
- resampling æ˜¯å¦æŠŠ posterior å¼ºè¡Œé›†ä¸­åˆ°æŸäº›ç²’å­
- æ˜¯å¦ä½¿ç”¨äº†é”™è¯¯çš„ maskï¼ˆsvd_mask / forward_op.M / S / S_safeï¼‰

### B. èƒ½å¤Ÿè®¡ç®—ç†è®º posterior variance

ç»™å®šï¼š
- prior covarianceï¼šblock-diagonal
- A = M @ F çš„ SVD
- noise std = 0.5

ä½ éœ€è¦è‡ªåŠ¨ç”Ÿæˆï¼š
- posterior covariance Î£_post
- Î£_post åœ¨ SVD åæ ‡ä¸‹çš„å¯¹è§’çº¿ï¼ˆvar_zï¼‰
- true ratio: mean(var_null) / mean(var_obs)

å¹¶æŠŠå®ƒæ‰“å°å‡ºæ¥ç”¨äºå¯¹æ¯”ã€‚

### C. Debug è¡Œä¸ºå‡†åˆ™

å½“æˆ‘è¯´"ç»§ç»­"æ—¶ï¼Œä½ è¦æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1. **é‡æ–°æ‰“å° MCG-diff æ¨ç†æµç¨‹ä¸­ä¸ nullspace variance ç›¸å…³çš„å…¨éƒ¨å˜é‡**ï¼š
   - x_t æ›´æ–°
   - x_unmasked æ›´æ–°
   - x_masked æ›´æ–°
   - log_prob
   - gather indices
   - svd_mask, S, S_safe

2. **è‡ªåŠ¨æ£€æŸ¥ä»¥ä¸‹é”™è¯¯æ¨¡å¼**ï¼š

| é”™è¯¯ç±»å‹ | åˆ¤å®šæ–¹å¼ |
|---------|---------|
| nullspace è¢«è¯¯å½“ä½œ observed | null dims å‡ºç°åœ¨ likelihood çº¦æŸé‡Œ |
| score å¼ºè¡Œä½œç”¨åœ¨ nullspace | denoised_t ä½¿ç”¨äº†é”™è¯¯ broadcast |
| resampling æ„å¤–æ”¶ç¼© nullspace | multinomial é€‰æ‹©åªé›†ä¸­åœ¨å°‘æ•°ç²’å­ |
| åˆå§‹åŒ–ä¸ç¬¦åˆ prior | sigma_max åˆå§‹åŒ–å¯¼è‡´ nullspace variance å¤ªå° |

3. **é’ˆå¯¹æ£€æµ‹åˆ°çš„é—®é¢˜ï¼Œæä¾›ç²¾ç¡®çš„ä»£ç  patch**ï¼Œä¾‹å¦‚ï¼š

```python
# WRONG
x_unmasked = x_next_t + sqrt(factor) * randn

# FIX
x_unmasked = x_t + sqrt(factor) * randn   # pure prior reverse diffusion
```

4. **è¿è¡Œ"patch å MCG-diff"çš„é¢„æœŸè¡Œä¸ºå¯¹æ¯”**ï¼ˆä¸ç”¨å®é™…è¿è¡Œï¼Œç»™å‡ºç†è®ºé¢„æœŸï¼‰

### D. æœ€ç»ˆç›®æ ‡ï¼š

å½“æˆ‘è¯´ï¼š"è¯·ç¡®è®¤ nullspace variance è¾¾åˆ°ç†è®ºå€¼"

ä½ è¦æ£€æŸ¥ï¼š
- è¾“å‡ºä¸­ null dims çš„ variance æ˜¯å¦ â‰ˆ prior varianceï¼ˆ2â€“5ï¼‰
- è¾“å‡ºä¸­ observed dims çš„ variance æ˜¯å¦ â‰ˆ 0.1â€“0.3
- ratio æ˜¯å¦ â‰¥ 5

å¹¶å‘Šè¯‰æˆ‘"MCG-diff æ˜¯å¦å·²æ¢å¤æ­£ç¡®çš„æ¬ å®šæ€§ä¸ç¡®å®šæ€§å»ºæ¨¡"ã€‚

---

## ğŸ“‹ ä»£ç ä¸Šä¸‹æ–‡

### 1. MCG_diff.inference() å®Œæ•´ä»£ç 

```python
import torch
import tqdm
from .base import Algo
import numpy as np
from utils.scheduler import Scheduler
from utils.helper import has_svd

class MCG_diff(Algo):
    def __init__(self, 
                 net,
                 forward_op,
                 scheduler_config,
                 num_particles):
        super(MCG_diff, self).__init__(net, forward_op)
        assert has_svd(forward_op), "MCG_diff only works with linear forward operators, which can be decomposed via SVD"
        self.scheduler = Scheduler(**scheduler_config)
        self.num_particles = num_particles

    def score(self, model, x, sigma):
        sigma = torch.as_tensor(sigma).to(x.device)
        d = model(x, sigma)
        return (d - x) / sigma**2
    
    def K(self, t):
        if t == self.scheduler.num_steps:
            return 1
        return self.scheduler.factor_steps[t] / (self.scheduler.factor_steps[t]+ self.scheduler.sigma_steps[t]**2)
    
    @torch.no_grad()
    def inference(self, observation, num_samples=1, **kwargs):
        device = self.forward_op.device
        observation = observation / self.forward_op.unnorm_scale - self.forward_op.forward(self.forward_op.unnorm_shift * torch.ones(num_samples, self.net.img_channels, self.net.img_resolution, self.net.img_resolution, device=device),unnormalize=False)

        # Mask for observed SVD dimensions
        S = self.forward_op.S.to(device)
        # For MRI-like A, S should be exactly 0 or 1, but numerical errors may produce
        # values close to 0 (e.g., 1e-8). Use a stricter threshold (0.1) to only keep
        # dimensions where S is close to 1, avoiding numerical explosion when dividing.
        # This ensures we only use well-conditioned dimensions.
        svd_mask = (S > 0.1).float()

        # Compute observation_t = Ut(y) / S  for observed dims only
        obs_ut = self.forward_op.Ut(observation)
        
        # Clip S to avoid numerical explosion when dividing by very small values
        # For MRI-like A, S should be 0 or 1, so clip small values to a safe minimum
        # Use 0.1 as minimum to avoid dividing by values that cause explosion
        # (For S < 0.1, the dimension should be treated as unobserved anyway)
        S_clipped = torch.clamp(S, min=0.1)  # Clip S to minimum 0.1
        S_safe = torch.where(svd_mask > 0, S_clipped, torch.ones_like(S))
        observation_t = (obs_ut / S_safe) * svd_mask
        
        # Initialize x_t in SVD space
        z = torch.randn(num_samples, self.num_particles, *self.forward_op.M.shape, device=device)
        K0 = self.K(0)  # K(0) = 1
        x_t = self.scheduler.sigma_max * z
        
        pbar = tqdm.trange(self.scheduler.num_steps)

        MAX_BATCH_SIZE = 128
        for step in pbar:
            sigma, sigma_next, factor, scaling_factor, scaling_step = self.scheduler.sigma_steps[step], self.scheduler.sigma_steps[step + 1], self.scheduler.factor_steps[step], self.scheduler.scaling_factor[step], self.scheduler.scaling_steps[step]
            x = self.forward_op.V(x_t.flatten(0,1))

            denoised_t = []
            for i in range(0, x.shape[0], MAX_BATCH_SIZE):
                # Follow reference implementation: Vt(...).view(-1, num_particles, *M.shape)
                denoised_t.append(self.forward_op.Vt(self.net(x[i:i+MAX_BATCH_SIZE]/scaling_step, torch.as_tensor(sigma).to(x.device))).view(-1, self.num_particles, *self.forward_op.M.shape))
            denoised_t = torch.cat(denoised_t, dim=0)
            score = (denoised_t - x_t / scaling_step) / sigma ** 2 / scaling_step
            x_next_t = x_t * scaling_factor + factor * score
            
            # Compute log probability for resampling (follow reference implementation)
            # Use svd_mask consistently for all likelihood terms
            log_prob = -torch.linalg.norm(
                ((observation_t - x_next_t) * svd_mask).flatten(2),
                dim=-1
            )**2 / (2 * (sigma_next**2 + factor))
            # FIXED: Changed from self.forward_op.M to svd_mask for consistency
            log_prob += torch.linalg.norm(((observation_t - x_t) * svd_mask).flatten(2), dim=-1)**2 / (2 * sigma**2)
            
            log_prob -= log_prob.min(dim=1, keepdim=True)[0]
            log_prob = torch.clamp(log_prob, max=60)
            # Ensure numerical stability: clamp to avoid exp(very negative) = 0, and ensure no NaN/Inf
            log_prob = torch.clamp(log_prob, min=-700, max=60)  # exp(-700) is approximately 0, safe for float32
            # Check for NaN/Inf and replace with very negative value
            log_prob = torch.where(torch.isfinite(log_prob), log_prob, torch.tensor(-700.0, device=log_prob.device))
            # Compute probabilities and ensure they are valid
            prob = torch.exp(log_prob)
            # Normalize to ensure valid probability distribution
            prob_sum = prob.sum(dim=1, keepdim=True)
            # If sum is too small (all probabilities near zero), use uniform distribution as fallback
            uniform_prob = torch.ones_like(prob) / self.num_particles
            prob = torch.where(prob_sum > 1e-10, prob / prob_sum, uniform_prob)
            # Ensure all probabilities are non-negative and finite
            prob = torch.clamp(prob, min=0.0, max=1.0)
            prob = torch.where(torch.isfinite(prob), prob, uniform_prob)
            # Final normalization to ensure sum is exactly 1.0
            prob_sum = prob.sum(dim=1, keepdim=True)
            prob = torch.where(prob_sum > 1e-10, prob / prob_sum, uniform_prob)
            # Final check: ensure sum is valid before multinomial
            prob_sum = prob.sum(dim=1, keepdim=True)
            if (prob_sum <= 0).any():
                # Fallback to uniform if any batch has invalid sum
                prob = torch.where(prob_sum <= 0, uniform_prob, prob)
            indices = torch.multinomial(prob, self.num_particles, replacement=True)
            

            K = self.K(step+1)
            # Gather indices: x_next_t is [B, num_particles, *M.shape], indices is [B, num_particles]
            # Need to expand indices to match x_next_t's shape for gathering along dimension 1
            # x_next_t shape: [B, num_particles, *M.shape], e.g., [B, num_particles, 1, 1, 4, 4]
            # We need gather_indices with same shape as x_next_t
            gather_indices = indices.unsqueeze(-1)  # [B, num_particles, 1]
            # Add trailing dimensions: need len(M.shape) - 1 more dims (since we already have 1)
            # x_next_t is [B, num_particles] + M.shape, so we need indices: [B, num_particles] + M.shape
            # From [B, num_particles, 1], we need to add the remaining dims from M.shape
            for _ in range(len(self.forward_op.M.shape) - 1):
                gather_indices = gather_indices.unsqueeze(-1)
            # Now expand the last dimensions to match M.shape[1:] (skip the first dim of M which is batch-like)
            # gather_indices is [B, num_particles, 1, 1, 1, ...], need to expand to match x_next_t
            gather_indices = gather_indices.expand(list(gather_indices.shape[:2]) + list(x_next_t.shape[2:]))
            x_next_t = torch.gather(x_next_t, 1, gather_indices)
            
            # Update x_t using masked and unmasked updates (follow reference implementation)
            # Use svd_mask consistently (already defined at the beginning)
            x_masked = (
                K * observation_t * svd_mask +
                (1 - K) * x_next_t + 
                np.sqrt(K) * sigma_next * torch.randn_like(x_t)
            )
            x_unmasked = x_next_t + np.sqrt(factor) * torch.randn_like(x_t)
            # pure prior reverse diffusion, not guided by likelihood or score
            # x_unmasked = x_t + np.sqrt(factor) * torch.randn_like(x_t)

            x_t = svd_mask * x_masked + (1 - svd_mask) * x_unmasked
            
        # Return final result: convert from SVD space to image space
        # MCG_diff particles are internal Monte Carlo objects for guidance, NOT posterior samples
        # Each inference() call should return exactly ONE posterior sample
        # True posterior sampling is done by multiple independent inference() calls
        # x_t shape: [B=1, num_particles=P, *M.shape]
        
        if num_samples == 1:
            # Choose ONE particle as the posterior sample (not average, not all particles)
            # Use first particle for deterministic behavior, or random for stochastic
            # x_t[0] has shape [num_particles, *M.shape]
            # Select first particle: x_t[0, 0] -> [*M.shape]
            x_final_svd = x_t[0, 0:1]  # [1, *M.shape] - select first particle, keep batch dim
        else:
            # For multiple batches, select first particle from each batch
            # x_t shape: [B, num_particles, *M.shape]
            x_final_svd = x_t[:, 0:1]  # [B, 1, *M.shape] - select first particle from each batch
        
        # Convert to image space
        # x_final_svd: [1, *M.shape] or [B, 1, *M.shape]
        x_final_img = self.forward_op.V(x_final_svd)  # [1, 1, 4, 4] or [B, 1, 4, 4]
        return x_final_img
```

### 2. é—®é¢˜è®¾ç½®

**Prior X (å…ˆéªŒåˆ†å¸ƒ)**:
- 16ç»´æ··åˆé«˜æ–¯åˆ†å¸ƒ (Mixture of Gaussians, MoG)
- å‰8ç»´ (dim 0-7): MoGï¼Œ2ä¸ªåˆ†é‡ï¼Œå‡å€¼åœ¨ç¬¬7ç»´åˆ†åˆ«ä¸º-2.0å’Œ+2.0ï¼Œåæ–¹å·®æ˜¯Toeplitzç»“æ„ (rho=0.8)
- å8ç»´ (dim 8-15): å¼±é«˜æ–¯å…ˆéªŒï¼Œå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º5.0
- å…ˆéªŒåæ–¹å·®çŸ©é˜µï¼šå‰8Ã—8å—æ˜¯ToeplitzçŸ©é˜µï¼Œå8Ã—8å—æ˜¯å¯¹è§’çŸ©é˜µï¼ˆå¯¹è§’çº¿å…ƒç´ ä¸º5.0ï¼‰

**A (å‰å‘ç®—å­çŸ©é˜µ)**:
- ç±»å‹: MRI-like (A = M @ F)
- F: 16Ã—16 ç±»FourierçŸ©é˜µï¼ˆæ­£äº¤ï¼‰
- M: å¯¹è§’maskçŸ©é˜µï¼Œåªæœ‰9ä¸ª1ï¼Œ7ä¸ª0
- SVDåˆ†è§£:
  - è§‚æµ‹ç»´åº¦ (S > 0.1): [0, 1, 2, 3, 4, 5, 6, 7, 8] (å…±9ä¸ª)
  - Nullç©ºé—´ç»´åº¦ (S â‰¤ 0.1): [9, 10, 11, 12, 13, 14, 15] (å…±7ä¸ª)

**è§‚æµ‹æ¨¡å‹**: y = A @ x + noise, noise_std = 0.5

### 3. å½“å‰è¯Šæ–­ç»“æœ

ä» `debug_mcgdiff_nullspace.py` çš„è¾“å‡ºï¼š
- **SVDä¸€è‡´æ€§**: âœ… æ­£å¸¸
- **Nullspaceå­˜åœ¨**: âœ… 9ä¸ªobserved dimsï¼Œ7ä¸ªnull dims
- **MCG-diffè¿”å›**: âœ… æ ·æœ¬æœ‰æ–¹å·®
- **Nullspaceæ³„æ¼**: âœ… æ— æ³„æ¼
- **é—®é¢˜**: âŒ Nullspace variance ratio = 0.8072ï¼ˆæœŸæœ› >> 1ï¼Œç†æƒ³5-20ï¼‰

**å…³é”®å‘ç°**:
- è§‚æµ‹ç»´åº¦æ–¹å·®: 0.1588
- Nullç©ºé—´æ–¹å·®: 0.1282
- Ratio: 0.8072ï¼ˆåº”è¯¥ >> 1ï¼‰

### 4. ç†è®ºé¢„æœŸ

å¯¹äºçº¿æ€§é€†é—®é¢˜ y = Ax + noiseï¼ŒåéªŒåæ–¹å·®ä¸ºï¼š
- Î£_post = (A^T A / Ïƒ_noise^2 + Î£_prior^-1)^-1

åœ¨SVDç©ºé—´ï¼ˆz = V^T xï¼‰ä¸­ï¼š
- Observed dims: variance â‰ˆ Ïƒ_noise^2 / S^2ï¼ˆå½“S=1æ—¶ï¼Œâ‰ˆ 0.25ï¼‰
- Null dims: variance â‰ˆ prior varianceï¼ˆâ‰ˆ 5.0 for dims 8-15ï¼‰

**é¢„æœŸ ratio**: mean(var_null) / mean(var_obs) â‰ˆ 5.0 / 0.25 = 20

---

## âœ¨ å¼€å§‹è°ƒè¯•

è¯·æŒ‰ç…§ä¸Šè¿°æ­¥éª¤å¼€å§‹åˆ†æ nullspace variance collapse çš„åŸå› ã€‚

**ç¬¬ä¸€æ­¥**: è¯·åˆ†æä»£ç ï¼Œæ‰¾å‡ºæ‰€æœ‰å¯èƒ½å¯¼è‡´ nullspace variance è¢«æŠ‘åˆ¶çš„åœ°æ–¹ã€‚

**ç¬¬äºŒæ­¥**: è®¡ç®—ç†è®ºåéªŒæ–¹å·®ï¼Œå¹¶ä¸å½“å‰è¾“å‡ºå¯¹æ¯”ã€‚

**ç¬¬ä¸‰æ­¥**: æä¾›ç²¾ç¡®çš„ä»£ç  patchã€‚

---

**æç¤º**: å½“ä½ å‡†å¤‡å¥½æ—¶ï¼Œè¯´"ç»§ç»­"ï¼Œæˆ‘ä¼šæä¾›æ›´å¤šè°ƒè¯•ä¿¡æ¯ã€‚
