diff --git a/algo/mcgdiff.py b/algo/mcgdiff.py
index 0000000..1111111 100644
--- a/algo/mcgdiff.py
+++ b/algo/mcgdiff.py
@@ -40,6 +40,48 @@ class MCG_diff(Algo):
     def K(self, t):
         if t == self.scheduler.num_steps:
             return 1
         return self.scheduler.factor_steps[t] / (self.scheduler.factor_steps[t]+ self.scheduler.sigma_steps[t]**2)
+    
+    def _reverse_step_prior(self, x_t, scaling_factor, factor, noise):
+        """
+        Pure prior reverse diffusion step for nullspace dimensions.
+        
+        This implements unconditional DDPM/DDIM reverse step with NO data guidance.
+        Uses the same noise schedule as conditional chain but without likelihood/score.
+        
+        Args:
+            x_t: Current state in SVD space [B, num_particles, *M.shape]
+            scaling_factor: Scaling factor from scheduler
+            factor: Factor from scheduler
+            noise: Shared random noise [B, num_particles, *M.shape]
+        
+        Returns:
+            x_prior: Next state following pure prior reverse diffusion
+        """
+        # Pure prior reverse diffusion: x_{t-1} = scaling_factor * x_t + sqrt(factor) * noise
+        # No score, no data guidance - just prior-driven diffusion
+        x_prior = x_t * scaling_factor + np.sqrt(factor) * noise
+        return x_prior
+    
+    def _reverse_step_conditional(self, x_t, x_next_t, observation_t, svd_mask, K, sigma_next, noise):
+        """
+        Conditional reverse diffusion step for observed dimensions.
+        
+        This implements data-guided reverse step using measurement y.
+        
+        Args:
+            x_t: Current state in SVD space [B, num_particles, *M.shape]
+            x_next_t: Score-guided next state [B, num_particles, *M.shape]
+            observation_t: Observation in SVD space [B, 1, *M.shape] or [1, 1, *M.shape]
+            svd_mask: Mask for observed dimensions [1, 1, *M.shape]
+            K: Kalman gain factor
+            sigma_next: Next noise level
+            noise: Shared random noise [B, num_particles, *M.shape]
+        
+        Returns:
+            x_post: Next state following conditional reverse diffusion
+        """
+        # Conditional update: blend observation with score-guided prediction
+        # x_post = K * observation + (1-K) * x_next_t + sqrt(K) * sigma_next * noise
+        x_post = (
+            K * observation_t * svd_mask +
+            (1 - K) * x_next_t + 
+            np.sqrt(K) * sigma_next * noise
+        )
+        return x_post
     
     @torch.no_grad()
     def inference(self, observation, num_samples=1, **kwargs):
@@ -130,7 +172,6 @@ class MCG_diff(Algo):
         print("="*80 + "\n")
         
         pbar = tqdm.trange(self.scheduler.num_steps)
-
         MAX_BATCH_SIZE = 128
         for step in pbar:
             sigma, sigma_next, factor, scaling_factor, scaling_step = self.scheduler.sigma_steps[step], self.scheduler.sigma_steps[step + 1], self.scheduler.factor_steps[step], self.scheduler.scaling_factor[step], self.scheduler.scaling_steps[step]
@@ -195,25 +236,30 @@ class MCG_diff(Algo):
             gather_indices = gather_indices.expand(list(gather_indices.shape[:2]) + list(x_next_t.shape[2:]))
             x_next_t = torch.gather(x_next_t, 1, gather_indices)
             
-            # Update x_t using masked and unmasked updates (follow reference implementation)
-            # Use svd_mask consistently (already defined at the beginning)
-            x_masked = (
-                K * observation_t * svd_mask +
-                (1 - K) * x_next_t + 
-                np.sqrt(K) * sigma_next * torch.randn_like(x_t)
-            )
-            # NULLSPACE SHOULD FOLLOW PURE PRIOR REVERSE DIFFUSION
-            # CRITICAL FIX: Use x_t (not x_next_t) to avoid score contamination
-            # x_next_t contains denoiser score which should NOT affect nullspace
-            # Using x_next_t causes nullspace variance collapse (ratio ≈ 1 instead of ≈ 5-10)
-            x_unmasked = x_next_t + np.sqrt(factor) * torch.randn_like(x_t)
+            # ========================================================================
+            # CRITICAL FIX: Separate conditional and prior reverse diffusion chains
+            # ========================================================================
+            # Sample shared noise ONCE per step (both chains use same noise)
+            shared_noise = torch.randn_like(x_t)
+            
+            # 1. Observed dimensions: conditional reverse diffusion with data guidance
+            x_post = self._reverse_step_conditional(
+                x_t=x_t,
+                x_next_t=x_next_t,
+                observation_t=observation_t,
+                svd_mask=svd_mask,
+                K=K,
+                sigma_next=sigma_next,
+                noise=shared_noise
+            )
+            
+            # 2. Null dimensions: pure prior reverse diffusion (NO data guidance)
+            x_prior = self._reverse_step_prior(
+                x_t=x_t,
+                scaling_factor=scaling_factor,
+                factor=factor,
+                noise=shared_noise
+            )
+            
+            # 3. Combine: observed dims use x_post, null dims use x_prior
+            x_t = svd_mask * x_post + (1 - svd_mask) * x_prior
 
             x_t = svd_mask * x_masked + (1 - svd_mask) * x_unmasked
             
         # Return final result: convert from SVD space to image space
